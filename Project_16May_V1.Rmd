---
title: "Project: High Dimensional Data"
author: "Sunil Raut Kshetri, Zdenko Heyvaert, Matthias Van Limbergen, Tim Msc"
date: "5 June 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# An executive summary of about half a page... 

```{r}
load("X_GSE21374.rda")
GeneExp <-t(X_GSE21374)
load("RejectionStatus.rda")
table(RejectionStatus$Reject_Status)
```
206 accepted (0) and 76 rejected (1) cases.

## Research question 1:  explore whether the variability in gene expression levels is associated with rejection status (Zdenko writes about technical summary of this question)

### Scree plot
```{r}
GeneExp.svd <- svd(GeneExp)
nEig <- length(GeneExp.svd$d)
totvar <- sum(GeneExp.svd$d^2)/(nEig -1)
#par(mfrow = c(1,3))
plot(GeneExp.svd$d[1:100]^2/(nEig -1), type = "b", ylab = "eigenvalues", xlab = 'j')

barplot(GeneExp.svd$d[1:120]^2/(nEig-1)/totvar, names.arg = 1:120, ylab = "proportion variance", cex.lab = 1.5)

cSum <- GeneExp.svd$d^2

barplot(cumsum(cSum[1:120]/(nEig-1)/totvar), names.arg = 1:120,
        ylab = "cumulative proportion.", cex.lab = 1.5,
        ylim = c(0,1))
abline(h=0.8)

#par(mfrow = c(1,1))

```
Upto 80% variance is explained by the first 120 PCs out of total 282 PCs.


### Compute scores and loadings
```{r}
# 4. compute scores and loadings
k = 2
Uk <- GeneExp.svd$u[,1:k]
# loadings
Vk <- GeneExp.svd$v[,1:k]
Dk <- diag(GeneExp.svd$d[1:k])
# scores
Zk <- Uk %*% Dk
rownames(Zk) <- RejectionStatus$Reject_Status
```

### Visualization of scores in reduced dimension

```{r}
# 5. visualization of scores in reduced dimension
plot(Zk[,1], Zk[,2], main = "scores", type = "p", pch = 16,
     xlab = "Z1", ylab = "Z2", col = RejectionStatus$Reject_Status+1)
legend(-180, 180, legend = c( "accept", "reject"), bty = "n", lwd = 2, col = c(1,2), pch = c(16,16))
```
Observations seem to spread over score dimensions (Z1 and Z2 axes), i.e, no noticable separation between patients whose kidney transplantation were accepted versus rejected. 
However, several transplantations were successful for subjects having scores $Z1 \geq 100$ and $Z2 \leq 50$. 


---- ZDENKO ------

Because of the large amount of genes, it is computationally impossible to perform an LDA on the entire data set. Therefore, we first have to perform some kind of feature selection. We will use the theory of large scale hypothesis testing to select the features that have a signficant contribution.

```{r}
# load the data so the following cells can be run standalone
load("X_GSE21374.rda")
load("RejectionStatus.rda")
GeneExpression <- t(X_GSE21374)
rm(X_GSE21374)

# data matrx
X <- GeneExpression
rm(GeneExpression)

# class vector
Y <- RejectionStatus$Reject_Status
rm(RejectionStatus)
```

We start by calculating the $p$-values of all genes
```{r}
# the number of genes
cols <- ncol(X)

# initialize vector with p-values
p.values <- rep(NA, cols)

# calculate and store p-values of each gene
for (i in 1:cols) {
  test <- t.test(X[,i] ~ Y)
  p.values[i] <- test$p.value
  # if (i %% 10000 == 0) {
  #   # keep track of progress (loop takes a few minutes to complete)
  #  # print(i) 
  # }
}
rm(i)
```

Now we calculate the adjusted $p$-values using both the Benjamini & Hochberg and Bonferroni algorithms:
```{r}
p.values.bh <- p.adjust(p.values, method = "BH")
p.values.bon <- p.adjust(p.values, method = "bonferroni")
sum(p.values.bh < 0.05)/length(p.values) # 24%
sum(p.values.bon < 0.05)/length(p.values) # 3%
```
BH keeps 24\% of all genes as signficant (13531 features), Bonferroni only 3\% (1613 featues). Let's start off  performing LDA on the 13531 significant features according to the BH correction:

```{r}
# necessary package for lda() function
library(MASS)

# subselection of the data matrix 
Xr <- X[, p.values.bh < 0.05]

# performing LDA on the reduced data matrix
Xr.lda <- lda(Xr, grouping = Y) # takes ~1 minute to run

# matrix to transform observations to discriminant functions
V1 <- Xr.lda$scaling

Z1 <- Xr %*% V1
boxplot(Z1~Y, ylab = "LDA", xlab = "Rejected")

# calculate AUC
library(MLmetrics)
AUC(Z1,Y) # 94.5%
```

The boxplot appears to indicate that the values of $Z$ are different for the rejected and the non-rejected group. We can run a two-sample $t$-test to confirm whether this difference is significant:

```{r}
t.test(Z1,Y)$p.value
```
The difference is indeed significant at the 5\% level. Now for the Bonferroni corrected $p$-values:

```{r}
# subselection of the data matrix 
Xr <- X[, p.values.bon < 0.05]

# performing LDA on the reduced data matrix
Xr.lda <- lda(Xr, grouping = Y)

# matrix to transform observations to discriminant functions
V1 <- Xr.lda$scaling

Z1 <- Xr %*% V1
boxplot(Z1~Y, ylab = "LDA", xlab = "Rejected")

# calculate AUC
AUC(Z1,Y) # 96.8%
```

The AUC value is higher, even though we are considering fewer features in this case! The $p$-value again indicates that the difference between the boxplots is significant:

```{r}
t.test(Z1,Y)$p.value
```
We can look if we can reduce the number of features even further using sparse LDA.

```{r}
library(glmnet)
set.seed(45)
lda_loadings <- cv.glmnet(Xr, Z1, alpha = 0.5)
plot(lda_loadings)
```
Of the 1613 features found to be significant according to the Bonferroni correction, we should only keep $\approx 130$ in the model (1 SE away from minimum):

```{r}
sparse_lda_loadings <- as.vector(coef(lda_loadings, s = lda_loadings$lambda.1se))

# without intercept
SLDA <- Xr %*% sparse_lda_loadings[-1]
boxplot(SLDA~Y, ylab = "LDA", xlab = "Rejected")

# calculate AUC
AUC(SLDA,Y) # 97.7%
```
The AUC value has only slightly been reduced by taking only 10% of the features. The $p$-value indicating the separation between the two boxplots has been reduced.

```{r}
t.test(SLDA,Y)$p.value
```

We can perform hierarchial clustering on the data set with the remaining 133 features. We use a heatmap to explore the difference between the (hopefully two) distinct groups among the observations.

```{r}
# get the 133 features used for the sparse LDA
features <- coef(lda_loadings, s = lda_loadings$lambda.1se)
features <- features@i # indices of the non-zero features
Xrr <- Xr[, features]

# constructing the dendrogram
library(dendextend)
d_x <- dist(Xrr) # distance matrix
hc_x <- hclust(d_x, method = "complete") # hierarchial clustering
dend <- as.dendrogram(hc_x)

# color the branches based on the clusters:
dend <- color_branches(dend, k = 2)

library(colorspace) # get nice colors
library(viridis)
rejected_col <- (rainbow_hcl(2))[factor(Y)]

# manually match the labels, as much as possible, to the real classification of the patients:
labels_colors(dend) <- rainbow_hcl(2)[sort_levels_values(Y[order.dendrogram(dend)])]

gplots::heatmap.2((Xrr), 
                  main = paste("Selected", 
                               ncol(Xrr), 
                               "genes with significant contribution"),
                  dendrogram = "row",
                  Rowv = dend,
                  trace = "none",          
                  margins = c(2,2),      
                  density.info = "none",
                  RowSideColors = rejected_col,
                  col = rev(rainbow(2)), # viridis, #
                  labRow = FALSE, 
                  labCol = FALSE,
                  xlab = "selected genes",
                  ylab = "observations"
)
```


## Q2

### spliting data into a test (30%) and training (70%) dataset




```{r}
# load("X_GSE21374.rda")
# # Gene expression data
# GeneExp <-t(X_GSE21374)
 load("RejectionStatus.rda")
# split data
set.seed(123)
pTrain <- 0.7 # 70% training

# =====================================================
# # Without stratifing
# nTrain <- ceiling(pTrainData*nrow(GeneExp.std))
# trainID <- sample(282, nTrain)
# ====================================================

# =====================================================
# With stratifing
id.accept <- which(RejectionStatus$Reject_Status == 0)
id.reject <- which(RejectionStatus$Reject_Status == 1)
trainID.accept <- sample(id.accept, ceiling(pTrain*length(id.accept)))
trainID.reject <- sample(id.reject, floor(pTrain*length(id.reject)))
trainID <- c(trainID.accept, trainID.reject)
# ====================================================

# Training data
trainX <- GeneExp[trainID,]
trainY <- RejectionStatus$Reject_Status[trainID]

# Test data
testX <- GeneExp[-trainID,]
testY <- RejectionStatus$Reject_Status[-trainID]

# Summary of splited dataset
dataset.summary <- as.matrix(rbind(table(RejectionStatus$Reject_Status),
                                 table(trainY), table(testY)))
rownames(dataset.summary) <- c("All", "Train 70%", "Test 30%")
colnames(dataset.summary) <- c("Accepted", "Rejected")
dataset.summary       
```

### Principal Component Regression (PCR)
```{r}
X.train.svd <- svd(trainX)
U.train <- X.train.svd$u
D.train <- diag(X.train.svd$d)
Z.train <- U.train%*%D.train
V.train <- X.train.svd$v

dim(trainX)
dim(Z.train)
dim(V.train)
```

### scree plot of training dataset
```{r}
# scree plot
nEig <- length(X.train.svd$d)
totvar <- sum(X.train.svd$d^2)/(nEig -1)
#par(mfrow = c(1,3))
plot(X.train.svd$d[1:120]^2/(nEig -1), type = "b", ylab = "eigenvalues", xlab = 'j')

barplot(X.train.svd$d[1:120]^2/(nEig-1)/totvar, names.arg = 1:120, ylab = "proportion variance", cex.lab = 1.0)

cSum <- X.train.svd$d^2

barplot(cumsum(cSum[1:120]/(nEig-1)/totvar), names.arg = 1:120,
        ylab = "cumulative proportion", cex.lab = 1,
        ylim = c(0,1))
abline(h=0.8)
```
It seems 80% of total variance, in the training detaset, explained by first 89 PCs out of 198 PCs. Each PC is the linear combination of Genes expressions. So, we proceed our calculations to find out the mimum number of Gene expression levels for the number of PCcross validation with 89 PCs.


--------- Sunil ----------------

### Cost function for AUC

```{r}
# Cost function
AUC <- function(observedY, predictedY){
  AUC = auc(observedY, predictedY)
  return(AUC)
}
```


### Model building
```{r}
library(boot) # for cv.glm
# Model evaluation: Cross validation  
nPC=35 # > 35 starts producing warning message, AUC and PCs at nPC = 86 is the same as at nPC = 35
# for LOOCV K=sample size
# K=length(trainY) # error as Matthias aslo pointed out 
K = 5

set.seed(123)

cv.pcr.error = rep(0,nPC)#We store our errors here

for (i in 1:nPC){ 
  data=data.frame(trainY=trainY, Z.train[,1:i])
  cv.pcr.mod1=glm(trainY~.,data=data, family = "binomial")
  cv.pcr.error[i]=cv.glm(data, cv.pcr.mod1, cost = AUC,K=K)$delta[1]
  # cv.pcr.error[i]=cv.glm(Z.train[,1:i], trainY, family = "binomial", 
  #                        type.measure = "auc", K=K)$delta[1]
  #cat("PC 1 to ",i,"nn")
}

#Number of PCs at minimum MSE: 15 PCs
nPC_at_max_AUC_CV=c(1:nPC)[cv.pcr.error==max(cv.pcr.error)]
 
max_AUC_CV = cv.pcr.error[nPC_at_max_AUC_CV]
nPC_at_max_AUC_CV
max_AUC_CV
#Ploting results
plot(cv.pcr.error, ylab = "AUC",xlab="n PCs", main = paste(K, "fold CV"))
abline(v=nPC_at_max_AUC_CV, col = "red")
```
The maximum AUC 0.796 at 15th PCs (without stratified). With stratified samples, AUC 0.868 and 23 PCs. It seems that training dataset performs better for stratified sample. Let us check how well it perform in test dataset. We now use the loadings computed with the training dataset and test dataset to compute test scores. 

```{r}
# Test the model with the test dataset
Z.test <- as.matrix(testX)%*%V.train
testX <- data.frame(testX)


AUC_CV_PCR=max(cv.pcr.error)

# select number of PCs based on AUC
data=data.frame(Z.train[,1:nPC_at_max_AUC_CV])

# best model based on AUC evaluation measure
best_PCR_mod=glm(trainY~.,data=data,family="binomial")
# best model based on AUC for prediction 
datatest=data.frame(Z.test[,1:nPC_at_max_AUC_CV])

# predict probabilities with response option
predPCR_prob=predict(best_PCR_mod,newdata=datatest,type="response") 

# test how good the best model is doing?
AUC_test_PCR=AUC(testY,predPCR_prob)

AUC_test_PCR
AUC_CV_PCR

#Really close 
abs(AUC_CV_PCR-AUC_test_PCR)
```
AUC=0.827 for predicting Y based on 15 PCs (without stratified). AUC = 0.65 for predicting Y based on 23 PCs with stratified sample. With stratified sample, PCR does not perform well on prediction.  


## Ridge regression (copied codes from Matthias :)
```{r}
#Ridge regression
library(glmnet)
K = 5
set.seed(123)
ridge_mod=cv.glmnet(x = trainX, y = trainY,family="binomial",type.measure="auc",nfolds=K,alpha=0)
plot(ridge_mod)

#best gammas 
bestgamma.1se=ridge_mod$lambda.1se
bestgamma.min=ridge_mod$lambda.min
AUC_CV_ridge=max(ridge_mod$cvm)

#With the cv.glmnet function, 
#you can now choose the gamma in predict function
#we choose gamma min 
predridge_prob=predict(ridge_mod,newx=as.matrix(testX),s=bestgamma.min,type="response")

#how good are we doing on test? 
AUC_test_ridge=auc(testY,predridge_prob[,1])
AUC_test_ridge
AUC_CV_ridge
```
With test dataset (stratified sample), Ridge regression prediction yields AUC = 0.69. On the otherhand, with training dataset (stratified sample), the Ridge regression prediction gives AUC = 0.87. (Could you please check it?) 

## Lasso needs to be done
```{r}
#Lasso regression
#same as ridge but with different cv.glmnet function
set.seed(123)
lasso_mod = cv.glmnet(x = trainX, y = trainY,family="binomial",type.measure="auc",nfolds=K,alpha=1)
plot(lasso_mod)

#best lambdas 
bestlambda.1se=lasso_mod$lambda.1se
bestlambda.min=lasso_mod$lambda.min
AUC_CV_lasso=max(lasso_mod$cvm)

#With the cv.glmnet function, 
#you can now choose the gamma in predict function
#we choose gamma min 
predlasso_prob=predict(lasso_mod,newx=as.matrix(testX),s=bestlambda.min,type="response")

#how good are we doing on test? 
AUC_test_lasso=auc(testY,predlasso_prob[,1])
AUC_test_lasso
AUC_CV_lasso
```
With stratified samples, the Lasso seems to predict well with test dataset (AUC=0.75). In training dataset, it gives AUC=0.87.