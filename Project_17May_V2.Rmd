---
title: "Project: High Dimensional Data"
author: "Sunil Raut Kshetri, Zdenko Heyvaert, Matthias Van Limbergen, Tim Msc"
date: "5 June 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# An executive summary of about half a page... 

## Loading in the data
We construct the data matrix `X` and the class vector `Y`:
```{r}
load("X_GSE21374.rda")
load("RejectionStatus.rda")

# data matrix
X <- t(X_GSE21374)
rm(X_GSE21374)

# class vector
Y <- RejectionStatus$Reject_Status
rm(RejectionStatus)
```

## Descriptives
The classes are imbalanced, we should pay attention to this when splitting the data in training and test sets:
```{r}
table(Y)
```
The data matrix is already centered and scaled
```{r, fig.height = 3, fig.width = 5}
par(mfrow=c(1,2))
boxplot(colMeans(X), main = "means")
boxplot(apply(X,2,var), main = "variances")
```

## Research question 1:  explore whether the variability in gene expression levels is associated with rejection status

We start by performing an SVD on the data matrix to see whether we are able to visualize the variability in gene expression levels in a lower dimension:
```{r}
X.svd <- svd(X)
nEig <- length(X.svd$d)
totvar <- sum(X.svd$d^2)/(nEig -1)
```
Now we construct the scree plot, on the right we have the same plot but zoomed in on the first 5 dimensions:
```{r, fig.height = 3, fig.width = 8}
cSum <- X.svd$d^2

layout(matrix(c(rep(1,3),2), 1, 4, byrow = TRUE))
barplot(cumsum(cSum/(nEig-1)/totvar), names.arg = 1:length(cSum),
        ylab = "cumulative proportion", cex.lab = 1.5,
        ylim = c(0,1))
abline(h=0.8)

barplot(cumsum(cSum[1:5]/(nEig-1)/totvar), names.arg = 1:5,
        ylab = "cumulative proportion", cex.lab = 1.5)
abline(h=0.8)

rm(X.svd, cSum, nEig, totvar)
```
The first two PCs explain less than 25% of the variability in the gene expressions; up to 120 PCs are necessary to represent 80% of the variability. Therefore, representing the data in 2 dimensions using MDS is not an option to assess the true variability of the genes.

Because we are looking for the association between gene expression variability and rejection status, we propose a supervised approach like LDA. However, because of the large amount of genes, it is computationally impossible to perform an LDA on the entire data set. Therefore, we first have to perform some kind of feature selection. We will use the theory of large scale hypothesis testing to select the features that have a signficant association with the rejection status.

We start by calculating the $p$-values of all genes
```{r}
# the number of genes
cols <- ncol(X)

# initialize vector with p-values
p.values <- rep(NA, cols)

# calculate and store p-values of each gene
for (i in 1:cols) {
  test <- t.test(X[,i] ~ Y)
  p.values[i] <- test$p.value
  # if (i %% 10000 == 0) {
  #   # keep track of progress (loop takes a few minutes to complete)
  #  # print(i) 
  # }
}
rm(cols, i)
```

Now we calculate the adjusted $p$-values using both the Benjamini & Hochberg and Bonferroni algorithms:
```{r}
p.values.bh <- p.adjust(p.values, method = "BH")
p.values.bon <- p.adjust(p.values, method = "bonferroni")
sum(p.values.bh < 0.05)/length(p.values) # 24%
sum(p.values.bon < 0.05)/length(p.values) # 3%
rm(p.values)
```
BH keeps 24\% of all genes as signficant (13531 features), Bonferroni only 3\% (1613 featues). Let's start off  performing LDA on the 13531 significant features according to the BH correction:

```{r}
# necessary package for lda() function
library(MASS)

# subselection of the data matrix 
Xr <- X[, p.values.bh < 0.05]
rm(p.values.bh)

# performing LDA on the reduced data matrix
Xr.lda <- lda(Xr, grouping = Y) # takes ~1 minute to run

# matrix to transform observations to discriminant functions
V1 <- Xr.lda$scaling

# the first (and only) discriminant
Z1 <- Xr %*% V1
rm(V1, Xr, Xr.lda)

# calculate AUC
library(MLmetrics)
AUC(Z1,Y) # 94.5%
```
Let's visualize the separation between the two classes:
```{r, fig.height = 3, fig.width = 4}
library(ggplot2)

ggplot(data.frame(Status = factor(ifelse(Y==1,"rejected","accepted")), Z1 = Z1),
       aes(x = Status, y = Z1)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(aes(color = Status), position = position_jitter(0.2), cex = 0.5) +
  theme_bw()
```
The boxplot appears to indicate that the values of $Z_1$ are different for the rejected and the non-rejected group, though the separation is not perfect. 

Now for the Bonferroni corrected $p$-values:
```{r}
# subselection of the data matrix 
Xr <- X[, p.values.bon < 0.05]
rm(p.values.bon)

# performing LDA on the reduced data matrix
Xr.lda <- lda(Xr, grouping = Y)

# matrix to transform observations to discriminant functions
V1 <- Xr.lda$scaling

# the first (and only) discriminant
Z1 <- Xr %*% V1
rm(V1, Xr.lda)

# calculate AUC
AUC(Z1,Y) # 96.8%
```

The AUC value is higher than for the BH corrected $p$-values, even though we are considering fewer features in this case!
```{r, fig.height = 3, fig.width = 4}
library(ggplot2)

ggplot(data.frame(Status = factor(ifelse(Y==1,"rejected","accepted")), Z1 = Z1),
       aes(x = Status, y = Z1)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(aes(color = Status), position = position_jitter(0.2), cex = 0.5) +
  theme_bw()
```
The separation looks better in this case. We can look if we can reduce the number of features even further using sparse LDA.
```{r, fig.height = 3.5, fig.width = 5}
library(glmnet)
set.seed(45)
lda_loadings <- cv.glmnet(Xr, Z1, alpha = 0.5)
plot(lda_loadings)
```
Of the 1613 features found to be significant according to the Bonferroni correction, we should only keep $\approx 130$ in the model (1 SE away from minimum):
```{r}
sparse_lda_loadings <- as.vector(coef(lda_loadings, s = lda_loadings$lambda.1se))

# without intercept
SLDA <- Xr %*% sparse_lda_loadings[-1]

# calculate AUC
AUC(SLDA,Y) # 97.7%
```
The AUC value has only slightly been reduced by taking only 10% of the features. Separation looks slightly worse but not as bad as for the BH corrected $p$-values, and we only use 133 features!
```{r, fig.height = 3, fig.width = 4}
library(ggplot2)

ggplot(data.frame(Status = factor(ifelse(Y==1,"rejected","accepted")), SLDA = SLDA),
       aes(x = Status, y = SLDA)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(aes(color = Status), position = position_jitter(0.2), cex = 0.5) +
  theme_bw()
```
In an attempt to visualize variability in the space of the remaining 133 features, we will present them in a heatmap, where a vertical bar denotes the class label of the observations.
Hierarchial clustering is performed to see if the clusters more or less coincide with the known class labels.
```{r}
suppressPackageStartupMessages(library(dendextend))

# get the 133 features used for the sparse LDA
features <- coef(lda_loadings, s = lda_loadings$lambda.1se)
features <- features@i # indices of the non-zero features
Xrr <- Xr[, features]

# constructing the dendrogram
library(dendextend)
d_x <- dist(Xrr) # distance matrix
hc_x <- hclust(d_x, method = "complete") # hierarchial clustering
dend <- as.dendrogram(hc_x)

# color the branches based on the clusters:
dend <- color_branches(dend, k = 2)

library(colorspace) # get nice colors
rejected_col <- (rainbow_hcl(2))[factor(Y)]

# manually match the labels, as much as possible, to the real classification of the patients:
labels_colors(dend) <- rainbow_hcl(2)[sort_levels_values(Y[order.dendrogram(dend)])]

gplots::heatmap.2((Xrr), 
                  main = paste("Selected", 
                               ncol(Xrr), 
                               "genes with significant contribution"),
                  dendrogram = "row",
                  Rowv = dend,
                  trace = "none",          
                  margins = c(2,2),      
                  density.info = "none",
                  RowSideColors = rejected_col,
                  col = rev(terrain.colors(2, alpha = 0.5)), # viridis,
                  labRow = FALSE, 
                  labCol = FALSE,
                  xlab = "selected genes",
                  ylab = "observations"
)
```
Each column in the heatmap corresponds to one of the 133 genes that were selected from the sparse LDA, each row is one of the 282 observed patients. For simplicity, we draw the heatmap with only two colors: green means the selected gene has an above-average expression for the given patient, white means it has a below-average expression for the given patient. Hierarchial clustering was performed on this heatmap, the resulting dendrogram as plotted at the left. We divided it up in two clusters: teal and pink, which we hope would correspond to the rejected and accepted patients, respectively. The true labels for each patient are shown in the vertical bar between the dendrogram and the heatmap, teal corresponding to the rejected patients and pink to the accepted patients.

From the heatmap it is clear that these genes play a role in whether a patient will accept or reject a kidney. The combination of below-average expression for the leftmost genes and an above-average expression for the rightmost genes is the signature of rejection, while the inverse is the signature of acceptance. There are some "misclassifications", patients that fall in another cluster than we would expect, though note that most of these occur in observations where the gene expressions for both the leftmost and rightmost genes are similar. Indeed, the pink cluster could be subdivided in two subclusters, one with gene expressions that really resemble acceptance behaviour, and one with a more intermediary regime where one indeed finds patients of both rejection groups.

## Q2

### spliting data into a test (30%) and training (70%) dataset




```{r}
GenExp <- X
rm(X)
# load("X_GSE21374.rda")
# # Gene expression data
# GeneExp <-t(X_GSE21374)
 load("RejectionStatus.rda")
# split data
set.seed(123)
pTrain <- 0.7 # 70% training

# =====================================================
# # Without stratifing
# nTrain <- ceiling(pTrainData*nrow(GeneExp.std))
# trainID <- sample(282, nTrain)
# ====================================================

# =====================================================
# With stratifing
id.accept <- which(RejectionStatus$Reject_Status == 0)
id.reject <- which(RejectionStatus$Reject_Status == 1)
trainID.accept <- sample(id.accept, ceiling(pTrain*length(id.accept)))
trainID.reject <- sample(id.reject, floor(pTrain*length(id.reject)))
trainID <- c(trainID.accept, trainID.reject)
# ====================================================

# Training data
trainX <- GeneExp[trainID,]
trainY <- RejectionStatus$Reject_Status[trainID]

# Test data
testX <- GeneExp[-trainID,]
testY <- RejectionStatus$Reject_Status[-trainID]

# Summary of splited dataset
dataset.summary <- as.matrix(rbind(table(RejectionStatus$Reject_Status),
                                 table(trainY), table(testY)))
rownames(dataset.summary) <- c("All", "Train 70%", "Test 30%")
colnames(dataset.summary) <- c("Accepted", "Rejected")
dataset.summary       
```

### Principal Component Regression (PCR)
```{r}
X.train.svd <- svd(trainX)
U.train <- X.train.svd$u
D.train <- diag(X.train.svd$d)
Z.train <- U.train%*%D.train
V.train <- X.train.svd$v

dim(trainX)
dim(Z.train)
dim(V.train)
```

### scree plot of training dataset
```{r}
# scree plot
nEig <- length(X.train.svd$d)
totvar <- sum(X.train.svd$d^2)/(nEig -1)
#par(mfrow = c(1,3))
plot(X.train.svd$d[1:120]^2/(nEig -1), type = "b", ylab = "eigenvalues", xlab = 'j')

barplot(X.train.svd$d[1:120]^2/(nEig-1)/totvar, names.arg = 1:120, ylab = "proportion variance", cex.lab = 1.0)

cSum <- X.train.svd$d^2

barplot(cumsum(cSum[1:120]/(nEig-1)/totvar), names.arg = 1:120,
        ylab = "cumulative proportion", cex.lab = 1,
        ylim = c(0,1))
abline(h=0.8)
```
It seems 80% of total variance, in the training detaset, explained by first 89 PCs out of 198 PCs. Each PC is the linear combination of Genes expressions. So, we proceed our calculations to find out the mimum number of Gene expression levels for the number of PCcross validation with 89 PCs.


--------- Sunil ----------------

### Cost function for AUC

```{r}
# Cost function
AUC <- function(observedY, predictedY){
  AUC = auc(observedY, predictedY)
  return(AUC)
}
```


### Model building
```{r}
library(boot) # for cv.glm
# Model evaluation: Cross validation  
nPC=35 # > 35 starts producing warning message, AUC and PCs at nPC = 86 is the same as at nPC = 35
# for LOOCV K=sample size
# K=length(trainY) # error as Matthias aslo pointed out 
K = 5

set.seed(123)

cv.pcr.error = rep(0,nPC)#We store our errors here

for (i in 1:nPC){ 
  data=data.frame(trainY=trainY, Z.train[,1:i])
  cv.pcr.mod1=glm(trainY~.,data=data, family = "binomial")
  cv.pcr.error[i]=cv.glm(data, cv.pcr.mod1, cost = AUC,K=K)$delta[1]
  # cv.pcr.error[i]=cv.glm(Z.train[,1:i], trainY, family = "binomial", 
  #                        type.measure = "auc", K=K)$delta[1]
  #cat("PC 1 to ",i,"nn")
}

#Number of PCs at minimum MSE: 15 PCs
nPC_at_max_AUC_CV=c(1:nPC)[cv.pcr.error==max(cv.pcr.error)]
 
max_AUC_CV = cv.pcr.error[nPC_at_max_AUC_CV]
nPC_at_max_AUC_CV
max_AUC_CV
#Ploting results
plot(cv.pcr.error, ylab = "AUC",xlab="n PCs", main = paste(K, "fold CV"))
abline(v=nPC_at_max_AUC_CV, col = "red")
```
The maximum AUC 0.796 at 15th PCs (without stratified). With stratified samples, AUC 0.868 and 23 PCs. It seems that training dataset performs better for stratified sample. Let us check how well it perform in test dataset. We now use the loadings computed with the training dataset and test dataset to compute test scores. 

```{r}
# Test the model with the test dataset
Z.test <- as.matrix(testX)%*%V.train
testX <- data.frame(testX)


AUC_CV_PCR=max(cv.pcr.error)

# select number of PCs based on AUC
data=data.frame(Z.train[,1:nPC_at_max_AUC_CV])

# best model based on AUC evaluation measure
best_PCR_mod=glm(trainY~.,data=data,family="binomial")
# best model based on AUC for prediction 
datatest=data.frame(Z.test[,1:nPC_at_max_AUC_CV])

# predict probabilities with response option
predPCR_prob=predict(best_PCR_mod,newdata=datatest,type="response") 

# test how good the best model is doing?
AUC_test_PCR=AUC(testY,predPCR_prob)

AUC_test_PCR
AUC_CV_PCR

#Really close 
abs(AUC_CV_PCR-AUC_test_PCR)
```
AUC=0.827 for predicting Y based on 15 PCs (without stratified). AUC = 0.65 for predicting Y based on 23 PCs with stratified sample. With stratified sample, PCR does not perform well on prediction.  


## Ridge regression (copied codes from Matthias :)
```{r}
#Ridge regression
library(glmnet)
K = 5
set.seed(123)
ridge_mod=cv.glmnet(x = trainX, y = trainY,family="binomial",type.measure="auc",nfolds=K,alpha=0)
plot(ridge_mod)

#best gammas 
bestgamma.1se=ridge_mod$lambda.1se
bestgamma.min=ridge_mod$lambda.min
AUC_CV_ridge=max(ridge_mod$cvm)

#With the cv.glmnet function, 
#you can now choose the gamma in predict function
#we choose gamma min 
predridge_prob=predict(ridge_mod,newx=as.matrix(testX),s=bestgamma.min,type="response")

#how good are we doing on test? 
AUC_test_ridge=auc(testY,predridge_prob[,1])
AUC_test_ridge
AUC_CV_ridge
```
With test dataset (stratified sample), Ridge regression prediction yields AUC = 0.69. On the otherhand, with training dataset (stratified sample), the Ridge regression prediction gives AUC = 0.87. (Could you please check it?) 

## Lasso needs to be done
```{r}
#Lasso regression
#same as ridge but with different cv.glmnet function
set.seed(123)
lasso_mod = cv.glmnet(x = trainX, y = trainY,family="binomial",type.measure="auc",nfolds=K,alpha=1)
plot(lasso_mod)

#best lambdas 
bestlambda.1se=lasso_mod$lambda.1se
bestlambda.min=lasso_mod$lambda.min
AUC_CV_lasso=max(lasso_mod$cvm)

#With the cv.glmnet function, 
#you can now choose the gamma in predict function
#we choose gamma min 
predlasso_prob=predict(lasso_mod,newx=as.matrix(testX),s=bestlambda.min,type="response")

#how good are we doing on test? 
AUC_test_lasso=auc(testY,predlasso_prob[,1])
AUC_test_lasso
AUC_CV_lasso
```
With stratified samples, the Lasso seems to predict well with test dataset (AUC=0.75). In training dataset, it gives AUC=0.87.